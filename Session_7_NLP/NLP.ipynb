{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VAIBHAV2900/DLSS_6_Vision/blob/main/Session_7_NLP/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHygb0IsqYKc"
      },
      "source": [
        "# Deep Learning for Natural Language Processing\n",
        "Summer School 2021 \\\n",
        "Week 3 Session 2\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll9jGol4tpae"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFEwiWVVqXgL"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language.\n",
        "\n",
        "![Tokenizer](https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQjY-2gynj9Y",
        "outputId": "52cda2af-30a3-4e29-e32b-8cea5960bd5e"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "\n",
        "sentence = \"Try any sentence of your wish or don't!\"\n",
        "\n",
        "print( len(tokenizer(sentence)) )\n",
        "print( tokenizer.explain(sentence) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "[('TOKEN', 'Try'), ('TOKEN', 'any'), ('TOKEN', 'sentence'), ('TOKEN', 'of'), ('TOKEN', 'your'), ('TOKEN', 'wish'), ('TOKEN', 'or'), ('SPECIAL-1', 'do'), ('SPECIAL-2', \"n't\"), ('SUFFIX', '!')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH9-I58MravO"
      },
      "source": [
        "## Word Representation\n",
        "### GloVe Embeddings\n",
        "GloVe exploits the overall co occurrence statistics of the word corpus.\n",
        "* The resulting representations showcase interesting linear substructures of the word vector space.\n",
        "\n",
        "    - The Euclidean distance between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.\n",
        "\n",
        "[Download the pretrained weights.](https://nlp.stanford.edu/projects/glove/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCdpw5m_v5jP",
        "outputId": "303908b5-9dcb-4fda-baf4-60fbe40a28ef"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!mkdir embeddings\n",
        "!mkdir embeddings/glove.6B\n",
        "!unzip \"/content/glove.6B.zip\" -d \"/content/embeddings/glove.6B\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 07:33:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-07-22 07:33:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.28MB/s    in 2m 41s  \n",
            "\n",
            "2021-07-22 07:35:56 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: /content/embeddings/glove.6B/glove.6B.50d.txt  \n",
            "  inflating: /content/embeddings/glove.6B/glove.6B.100d.txt  \n",
            "  inflating: /content/embeddings/glove.6B/glove.6B.200d.txt  \n",
            "  inflating: /content/embeddings/glove.6B/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVPVn1DsycL"
      },
      "source": [
        "embeddings = {}\n",
        "with open(\"/content/embeddings/glove.6B/glove.6B.50d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = \" \".join(t for t in values[:-50])\n",
        "        vector = np.asarray(values[-50:], \"float64\")\n",
        "        embeddings[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQe9eWU4tyJF",
        "outputId": "68ccc860-367e-4982-d25d-4eac99515b13"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def find_closest_word(word):\n",
        "    embedding = embeddings[word]\n",
        "    return sorted(embeddings.keys(), key=lambda target: spatial.distance.euclidean(embeddings[target], embedding))\n",
        "\n",
        "closest = find_closest_word(\"king\")\n",
        "print(closest[:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['king', 'prince', 'queen', 'uncle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtZsvjw1ZdM"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "![Architecture](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
        "\n",
        "RNNs are a type of Neural Network architecture, where the output from the previous step is fed additionally as input the current step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDSQQ6X512sb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiIKcpk12LW"
      },
      "source": [
        "### From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmvxfySu1icU"
      },
      "source": [
        "class RNN_scratch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_tensor):\n",
        "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        hidden = self.tanh(hidden)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.tanh(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpXNfQ_A3oL5"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9NnMimG_xzw"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqqGM7rZ4rLt",
        "outputId": "6e832927-57c9-45da-9ecb-0bdf65e22148"
      },
      "source": [
        "!gdown --id 1KAScc_mAbz5soyxeMVWvzy036lkwHY0F\n",
        "!unzip \"/content/embeddings.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KAScc_mAbz5soyxeMVWvzy036lkwHY0F\n",
            "To: /content/embeddings.zip\n",
            "2.18GB [00:14, 151MB/s]\n",
            "Archive:  /content/embeddings.zip\n",
            "   creating: /content/embeddings/glove.840B.300d/\n",
            "  inflating: /content/embeddings/glove.840B.300d/glove.840B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VfHpxeE9YnW",
        "outputId": "bd02b0aa-ba66-43f2-cd0b-173a9a47bb18"
      },
      "source": [
        "!gdown --id 1CalhcgxPu00vKQAHNba_8zPcsNrLExXx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CalhcgxPu00vKQAHNba_8zPcsNrLExXx\n",
            "To: /content/train.csv\n",
            "124MB [00:01, 90.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0ofyN8u-gdV",
        "outputId": "617d0776-64a0-4936-9cfa-9f0cb9285970"
      },
      "source": [
        "PATH = %pwd\n",
        "print(PATH)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "BL7Hintn4ljw",
        "outputId": "3e976dd7-67ba-448f-e096-94050a1e497e"
      },
      "source": [
        "main_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
        "print(main_df.shape)\n",
        "main_df = main_df.sample(n=main_df.shape[0])\n",
        "main_df = main_df[[\"question_text\", \"target\"]]\n",
        "main_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1306122, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>629935</th>\n",
              "      <td>Why is colloidal oatmeal beneficial in skincare?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780706</th>\n",
              "      <td>The magnitude of electric fields in an electro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423082</th>\n",
              "      <td>How can a god of kama be so vital at this stag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1134560</th>\n",
              "      <td>How mass media portray nihilism?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108481</th>\n",
              "      <td>What will happen to a LDS Apostle if he is a s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question_text  target\n",
              "629935    Why is colloidal oatmeal beneficial in skincare?       0\n",
              "780706   The magnitude of electric fields in an electro...       0\n",
              "423082   How can a god of kama be so vital at this stag...       0\n",
              "1134560                   How mass media portray nihilism?       0\n",
              "108481   What will happen to a LDS Apostle if he is a s...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bKn6y7E40d6",
        "outputId": "5074a50a-8292-4671-c3a0-a3a2c71a84a3"
      },
      "source": [
        "o_class = main_df.loc[main_df.target == 0, :]\n",
        "print(len(o_class))\n",
        "l_class = main_df.loc[main_df.target == 1, :]\n",
        "print(len(l_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1225312\n",
            "80810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kEkE-E799cz"
      },
      "source": [
        "test_o = o_class.iloc[:1024, :]\n",
        "test_l = l_class.iloc[:1024, :]\n",
        "\n",
        "valid_o = o_class.iloc[1024:32768, :]\n",
        "valid_l = l_class.iloc[1024:32768, :]\n",
        "\n",
        "train_o = o_class.iloc[32768:, :]\n",
        "train_l = l_class.iloc[32768:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndY-_FGy-GIr",
        "outputId": "d7445c2f-daa3-4dca-e153-542c4d237da7"
      },
      "source": [
        "train = pd.concat([train_o, train_l], axis=0)\n",
        "print(train.shape)\n",
        "\n",
        "valid = pd.concat([valid_o, valid_l], axis=0)\n",
        "print(valid.shape)\n",
        "\n",
        "test = pd.concat([test_o, test_l], axis=0)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1240586, 2)\n",
            "(63488, 2)\n",
            "(2048, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6_n2gRM47aP"
      },
      "source": [
        "!mkdir inputs\n",
        "\n",
        "train.to_csv(os.path.join(PATH, \"inputs/train.csv\"), index=False)\n",
        "test.to_csv(os.path.join(PATH, \"inputs/test.csv\"), index=False)\n",
        "valid.to_csv(os.path.join(PATH, \"inputs/valid.csv\"), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxsTElPJ5AAS"
      },
      "source": [
        "del main_df, train, test, valid, train_l, train_o, test_l, test_o, valid_l,valid_o, o_class, l_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufoxOe_3u_f"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fThGH6x3wpU"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import spacy\n",
        "import nltk\n",
        "import torchtext\n",
        "\n",
        "class CreateDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, PATH, batch_size=32):\n",
        "        self.PATH = PATH\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.spacy = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        self.TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=\"spacy\")\n",
        "        self.LABEL = torchtext.legacy.data.LabelField(dtype=torch.long, sequential=False)\n",
        "\n",
        "        self.initData()\n",
        "        self.initEmbed()\n",
        "\n",
        "        self.makeData()\n",
        "\n",
        "    def initData(self):\n",
        "        DATA = os.path.join(self.PATH, 'inputs/')\n",
        "\n",
        "        self.train_data, self.valid_data, self.test_data = torchtext.legacy.data.TabularDataset.splits(\n",
        "                        path=DATA, \n",
        "                        train=\"train.csv\", validation=\"valid.csv\", test=\"test.csv\", \n",
        "                        format=\"csv\", \n",
        "                        skip_header=True, \n",
        "                        fields=[('Text', self.TEXT), ('Label', self.LABEL)])\n",
        "\n",
        "    def initEmbed(self):\n",
        "        EMBED = os.path.join(self.PATH, \"embeddings/glove.840B.300d/glove.840B.300d.txt\")\n",
        "\n",
        "        self.TEXT.build_vocab(self.train_data,\n",
        "                         vectors=torchtext.vocab.Vectors(EMBED), \n",
        "                         max_size=20000, \n",
        "                         min_freq=10)\n",
        "        self.LABEL.build_vocab(self.train_data)\n",
        "\n",
        "    def makeData(self):\n",
        "        self.train_iterator, self.valid_iterator, self.test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
        "                        (self.train_data, self.valid_data, self.test_data), \n",
        "                        sort_key=lambda x: len(x.Text), \n",
        "                        batch_size=self.batch_size,\n",
        "                        device=self.device)\n",
        "\n",
        "    def lengthData(self):\n",
        "        return len(self.train_data), len(self.valid_data), len(self.test_data)\n",
        "    \n",
        "    def lengthVocab(self):\n",
        "        return len(self.TEXT.vocab), len(self.LABEL.vocab)\n",
        "\n",
        "    def freqLABEL(self):\n",
        "        return self.LABEL.vocab.freqs\n",
        "\n",
        "    def getData(self):\n",
        "        return self.train_iterator, self.valid_iterator, self.test_iterator\n",
        "\n",
        "    def getEmbeddings(self):\n",
        "        return self.TEXT.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA6yqp-23-Kf"
      },
      "source": [
        "### Model\n",
        "Using pytorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aMGWnZS4DBz"
      },
      "source": [
        "import torch\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        out = self.linear(hidden)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFVrLFrb4SiH"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl0J6GdQxjrv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRap9UtG4agY"
      },
      "source": [
        "!pip3 install pyprind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NZTQO5V5Ji9"
      },
      "source": [
        "dataset = CreateDataset(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUb4AsRm5MKN"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = dataset.getData()\n",
        "pretrained_embeddings = dataset.getEmbeddings()\n",
        "\n",
        "input_dim = dataset.lengthVocab()[0]\n",
        "embedding_dim = 300\n",
        "hidden_dim = 374\n",
        "output_dim = 2\n",
        "num_layers = 2\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYFzi335SDi"
      },
      "source": [
        "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "model.embedding.weight.data = pretrained_embeddings.to(device)\n",
        "class_weights = torch.tensor([1.0, 15.0]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbDtdiV5WBu"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrqxUp-O5Y8f"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GubYOq99sjPF"
      },
      "source": [
        "DRIVE = \"/content/drive/MyDrive/Projects/Clubs/Analytics/Coordinator/Summer School\" # Use your drive path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnSGFc_5ady"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
        "    correct = (ind == y).float()\n",
        "    acc = correct.sum()/float(len(correct))\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrZuaiRRBjYU"
      },
      "source": [
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "epoch_val_losses = []  \n",
        "accu_train_epoch = []\n",
        "accu_test_epoch = []\n",
        "accu_val_epoch = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns7UIyQ64UbD"
      },
      "source": [
        "import pyprind\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "    train_loss_batch = []\n",
        "    accu_train_batch = []\n",
        "\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch.Text).squeeze(0)\n",
        "\n",
        "        loss = criterion(predictions, batch.Label)\n",
        "\n",
        "        acc = binary_accuracy(predictions, batch.Label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "        train_loss_batch.append(loss)\n",
        "        accu_train_batch.append(acc)\n",
        "\n",
        "        bar.update()\n",
        "\n",
        "    epoch_train_losses.append(sum(train_loss_batch)/len(iterator))\n",
        "    accu_train_epoch.append(sum(accu_train_batch)/len(iterator))\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    val_loss_batch = []\n",
        "    accu_val_batch = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.Text).squeeze(0)\n",
        "            \n",
        "            loss = criterion(predictions, batch.Label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.Label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "            val_loss_batch.append(loss)\n",
        "            accu_val_batch.append(acc)\n",
        "\n",
        "            bar.update()\n",
        "\n",
        "        epoch_val_losses.append(sum(val_loss_batch)/len(iterator))\n",
        "        accu_val_epoch.append(sum(accu_val_batch)/len(iterator))\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHcJP5Kx4X_n"
      },
      "source": [
        "import gc\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_val_losses[-1],\n",
        "            }, os.path.join(DRIVE, 'Quora.pt'))\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3zoKIoDCap"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwYh2orXDFCA"
      },
      "source": [
        "plt.plot(epoch_train_losses, label='Training Loss')\n",
        "plt.plot(epoch_val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYDObSrGDF-D"
      },
      "source": [
        "plt.plot(accu_train_epoch, label='Accuracy Training')\n",
        "plt.plot(accu_val_epoch, label='Accuracy Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-JV8lhzDIhf"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(\"\\nTest Loss:\", test_loss)\n",
        "print(\"\\nTest Accuracy:\", test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}